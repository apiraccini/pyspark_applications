{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "b533a462",
   "metadata": {},
   "source": [
    "# Toy analysis on simulated data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "5f060ca8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-0.0349682218527132 [ 3.5977656   1.89014171 -2.45067194  5.37054442 -4.6054133 ]\n"
     ]
    }
   ],
   "source": [
    "# setup\n",
    "import numpy as np\n",
    "import scipy as sp\n",
    "import pandas as pd\n",
    "from scipy.linalg import solve_triangular\n",
    "\n",
    "from sklearn.datasets import make_regression\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "n = 10000\n",
    "p = 5\n",
    "\n",
    "# create simulated data\n",
    "np.random.seed(42)\n",
    "X = np.random.random_sample((n, p))\n",
    "betas_real = np.random.normal(0, 3, p+1)\n",
    "y = betas_real[0] + np.dot(X, betas_real[1:]) + np.random.normal(0, 0.5, n)\n",
    "\n",
    "# save as pandas dataframe\n",
    "xy = pd.DataFrame(X, columns = [\"x\" + str(i+1) for i in range(p)])\n",
    "xy[\"y\"] = y\n",
    "\n",
    "# obtain linear model coefficients\n",
    "lm = LinearRegression()\n",
    "lm.fit(X, y)\n",
    "print(lm.intercept_, lm.coef_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "ee36652c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "            <div>\n",
       "                <p><b>SparkSession - in-memory</b></p>\n",
       "                \n",
       "        <div>\n",
       "            <p><b>SparkContext</b></p>\n",
       "\n",
       "            <p><a href=\"http://DESKTOP-BDQVD8K.station:4040\">Spark UI</a></p>\n",
       "\n",
       "            <dl>\n",
       "              <dt>Version</dt>\n",
       "                <dd><code>v3.3.0</code></dd>\n",
       "              <dt>Master</dt>\n",
       "                <dd><code>local[*]</code></dd>\n",
       "              <dt>AppName</dt>\n",
       "                <dd><code>This Spark session</code></dd>\n",
       "            </dl>\n",
       "        </div>\n",
       "        \n",
       "            </div>\n",
       "        "
      ],
      "text/plain": [
       "<pyspark.sql.session.SparkSession at 0x1eb031602e0>"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# spark setup\n",
    "import os\n",
    "import sys\n",
    "\n",
    "os.environ['PYSPARK_PYTHON'] = sys.executable\n",
    "os.environ['PYSPARK_DRIVER_PYTHON'] = sys.executable\n",
    "os.environ[\"PYARROW_IGNORE_TIMEZONE\"] = \"1\"\n",
    "\n",
    "# spark session\n",
    "from pyspark.sql import SparkSession\n",
    "\n",
    "spark = SparkSession \\\n",
    "    .builder \\\n",
    "    .master('local[*]') \\\n",
    "    .appName('This Spark session') \\\n",
    "    .config('spark.sql.repl.eagerEval.enabled', True) \\\n",
    "    .config('spark.sql.execution.arrow.pyspark.enabled', False) \\\n",
    "    .getOrCreate()\n",
    "\n",
    "spark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "d67eea67",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Full dataset has 10000 rows.\n",
      "Columns: ['x1', 'x2', 'x3', 'x4', 'x5', 'y']\n"
     ]
    }
   ],
   "source": [
    "# read data\n",
    "df = spark.createDataFrame(xy)\n",
    "n = df.count()\n",
    "print('Full dataset has', n, 'rows.')\n",
    "print('Columns:', df.columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "2686f4ca",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- index: double (nullable = false)\n",
      " |-- X: vector (nullable = true)\n",
      " |-- y: double (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# utils\n",
    "from pyspark.ml.feature import VectorAssembler\n",
    "from pyspark.ml.evaluation import RegressionEvaluator\n",
    "from pyspark.sql.functions import col, pandas_udf, lit\n",
    "\n",
    "# preprocessing\n",
    "from pyspark.sql.functions import row_number, monotonically_increasing_id\n",
    "from pyspark.sql import Window\n",
    "\n",
    "df = df.withColumn(\"index\", row_number().over(Window.orderBy(monotonically_increasing_id()))-1)\n",
    "df = df.withColumn('index', df['index'].cast('double'))\n",
    "\n",
    "features = [c for c in df.columns if c not in ['index', 'y']]\n",
    "assembler = VectorAssembler(inputCols = features, outputCol = 'X')\n",
    "\n",
    "df = assembler.transform(df).select(\"index\", \"X\", \"y\")\n",
    "df.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "4c2cd2a9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Split 1 has 5000 rows.\n",
      "Split 2 has 5000 rows.\n"
     ]
    }
   ],
   "source": [
    "# obtain two equal splits\n",
    "s1 = df.limit(int(n/2))\n",
    "ns = s1.count()\n",
    "s2 = df.subtract(s1).limit(ns)\n",
    "\n",
    "print('Split 1 has', ns, 'rows.')\n",
    "print('Split 2 has', s2.count(), 'rows.')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f94cfef6",
   "metadata": {},
   "source": [
    "### Linear model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "01e87e9d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# functions for liner model estimation and prediction\n",
    "\n",
    "def map_lm(row):\n",
    "    \"\"\"Calculate x*t(x) and x*y for each row\"\"\"\n",
    "    x = [1]\n",
    "    x.extend(row['X'].toArray())\n",
    "    x = np.array(x)\n",
    "    y = row[\"y\"]\n",
    "    out = [np.outer(x, x), x*y]    \n",
    "    return out\n",
    "\n",
    "def get_prediction_lm(row, beta):\n",
    "    \"\"\"Calculate predictions for a row from estimated linear model\"\"\"\n",
    "    x = row[\"X\"].toArray()\n",
    "    ypred = beta[1] + np.dot(x, beta[1:])\n",
    "    return (row + (float(ypred),))\n",
    "\n",
    "def cholesky_normal_equations(xtx, xty):\n",
    "    \"\"\"Calculate the solution to the normal equation via Cholesky scomposition\"\"\"\n",
    "    \n",
    "    L = np.linalg.cholesky(xtx) \n",
    "    z = solve_triangular(L, xty, lower = True) \n",
    "    beta_hat = solve_triangular(L, z, lower = True, trans = \"T\")\n",
    "    return beta_hat    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "9a2d35a8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: total: 15.6 ms\n",
      "Wall time: 16.2 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "# calculate t(X)X and t(X)y for the two splits\n",
    "xtx_lm_s1, xty_lm_s1 = s1.rdd.map(lambda row: map_lm(row)).reduce(lambda a,b: (a[0]+b[0], a[1]+b[1]))\n",
    "xtx_lm_s2, xty_lm_s2 = s2.rdd.map(lambda row: map_lm(row)).reduce(lambda a,b: (a[0]+b[0], a[1]+b[1]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "c14d2ad1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# solve normal equations\n",
    "beta_hat_lm_s1 = cholesky_normal_equations(xtx_lm_s1, xty_lm_s1)\n",
    "beta_hat_lm_s2 = cholesky_normal_equations(xtx_lm_s2, xty_lm_s2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "9922d15c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: total: 15.6 ms\n",
      "Wall time: 3.88 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "# calculate predictions from linear model\n",
    "o1 = s1.rdd.map(lambda row:get_prediction_lm(row, beta_hat_lm_s2)).toDF(['index', \"X\", \"y\", \"yhat\"])\n",
    "o2 = s2.rdd.map(lambda row:get_prediction_lm(row, beta_hat_lm_s1)).toDF(['index', \"X\", \"y\", \"yhat\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "0d52a7ad",
   "metadata": {},
   "outputs": [],
   "source": [
    "# functions for conformal prediction\n",
    "\n",
    "@pandas_udf(\"double\")\n",
    "def get_abs_error(yreal: pd.Series, ypred: pd.Series) -> pd.Series:\n",
    "    \"\"\"Calculate absolute prediction error\"\"\"\n",
    "    return np.abs(yreal-ypred)\n",
    "\n",
    "@pandas_udf(\"double\")\n",
    "def find_quantile(err: pd.Series) -> pd.Series:\n",
    "    \"\"\"given ordered absolute errors, calculate ROO empirical quantile\"\"\"\n",
    "    level = 0.95\n",
    "    m = int(np.ceil(len(err)*level))\n",
    "    out = [err.drop([i]).values[m-1] for i in range(len(err))]\n",
    "    return pd.Series(out)\n",
    "\n",
    "def ROOsplitconformal(df):\n",
    "    \"\"\"\n",
    "    given a Spark dataframe with columns y and yhat,\n",
    "    calculate and sort absolute errors, \n",
    "    find ROO empirical quantiles,\n",
    "    calculate lower and upper bounds for ROO split conformal prediction intervals    \n",
    "    \"\"\"\n",
    "    out = df \\\n",
    "            .withColumn(\"abs_error\", get_abs_error(col(\"y\"), col(\"yhat\"))).sort(col(\"abs_error\")) \\\n",
    "            .withColumn(\"ROO_eq\", find_quantile(col(\"abs_error\"))) \\\n",
    "            .withColumn(\"lower\", col(\"yhat\")-col(\"ROO_eq\")).withColumn(\"upper\", col(\"yhat\")+col(\"ROO_eq\"))\n",
    "    return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "c7405fb4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ROO split conformal predictive intervals\n",
    "o1 = o1.transform(ROOsplitconformal)\n",
    "o2 = o2.transform(ROOsplitconformal)\n",
    "\n",
    "# merge the splits\n",
    "res_lm = o1.union(o2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "e9268612",
   "metadata": {},
   "outputs": [],
   "source": [
    "# functions for results evaluation\n",
    "\n",
    "@pandas_udf(\"double\")\n",
    "def rmse(yreal: pd.Series, ypred: pd.Series) -> float:\n",
    "    return np.sqrt(np.mean((yreal-ypred)**2))\n",
    "\n",
    "@pandas_udf(\"double\")\n",
    "def rmse_orig(yreal: pd.Series, ypred: pd.Series) -> float:\n",
    "    return np.sqrt(np.mean((np.exp(yreal)-np.exp(ypred))**2))\n",
    "\n",
    "@pandas_udf(\"double\")\n",
    "def coverage(yreal: pd.Series, lower: pd.Series, upper: pd.Series) -> float:\n",
    "    return np.mean(((yreal>lower) & (yreal<upper))*1.0)\n",
    "\n",
    "@pandas_udf(\"double\")\n",
    "def avg_length(lower: pd.Series, upper: pd.Series) -> float:\n",
    "    return np.mean(upper - lower)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "247be895",
   "metadata": {},
   "outputs": [],
   "source": [
    "# evaluate results\n",
    "metrics_lm = res_lm.select(lit('linear').alias(\"model\"),\n",
    "                           rmse(col('y'), col('yhat')).alias('rmse'),\n",
    "                           coverage(col('y'), col('lower'), col('upper')).alias('coverage'), \n",
    "                           avg_length(col('lower'), col('upper')).alias('avg_length'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "81a08866",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+-----------------+--------+-----------------+\n",
      "| model|             rmse|coverage|       avg_length|\n",
      "+------+-----------------+--------+-----------------+\n",
      "|linear|3.665820561412509|    0.95|8.922321162283405|\n",
      "+------+-----------------+--------+-----------------+\n",
      "\n",
      "CPU times: total: 0 ns\n",
      "Wall time: 20.5 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "metrics_lm.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "930e8e52",
   "metadata": {},
   "source": [
    "### Additive model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "071ff57c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# functions for additive model estimation and prediction\n",
    "\n",
    "knots = np.linspace(0.05, 0.95, 10)\n",
    "\n",
    "def ncs_elem(x, xk):\n",
    "    \"\"\"given a feature and knots, calulates basis functtion\"\"\"\n",
    "    out = (((xk-1/2)**2-1/12)*((x-1/2)**2)-1/12)/4 - ((np.abs(x-xk)-1/2)**4-1/2*(np.abs(x-xk)-1/2)**2 + 7/240)/24\n",
    "    return out\n",
    "\n",
    "def splS(knots):\n",
    "    \"\"\"define S penalty matrix for a single term\"\"\"\n",
    "    q = len(knots)+2\n",
    "    S = np.zeros([q,q])\n",
    "    k = np.array(knots)\n",
    "    S[2:, 2:] = ncs_elem(k[:,None], k)\n",
    "    return S\n",
    "    \n",
    "def get_bigS(knots, lam = 0.01):\n",
    "    \"\"\"Obtain global penalty matrix\"\"\"\n",
    "    Slist = []\n",
    "    for i in range(p):\n",
    "        q = len(knots)+1\n",
    "        S = np.zeros([p*q, p*q])\n",
    "        S2 = np.zeros([len(S)+1, len(S)+1])\n",
    "        S[i*q:(i+1)*q, i*q:(i+1)*q] = splS(knots)[1:,1:]\n",
    "        S2[1:,1:] = S\n",
    "        Slist.append(S2)\n",
    "    bigS = sum(lam*elem for elem in Slist)\n",
    "    return bigS\n",
    "\n",
    "def map_gam(row):\n",
    "    \"\"\"for each row, compute natural cubic splines basis expansion for x then calculate x*t(x) and x*y\"\"\"\n",
    "    nx = [1]\n",
    "    x = row[\"X\"].toArray()\n",
    "    y = row[\"y\"]\n",
    "    bss = []\n",
    "    for xij in x:\n",
    "        nx.append(xij)\n",
    "        nx.extend([ncs_elem(xij, knot) for knot in knots])\n",
    "    x = np.array(nx)\n",
    "    out = [np.outer(x, x), x*y]\n",
    "    return out\n",
    "\n",
    "def get_prediction_gam(row, beta):\n",
    "    \"\"\"Calculate predictions for estimated additive model\"\"\"\n",
    "    nx = []\n",
    "    x = row[\"X\"].toArray()\n",
    "    bss = []\n",
    "    for xij in x:\n",
    "        nx.append(xij)\n",
    "        nx.extend([ncs_elem(xij, knot) for knot in knots])\n",
    "    x = np.array(nx)\n",
    "    ypred = beta[1] + np.dot(x, beta[1:])\n",
    "    return (row + (float(ypred),))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "29ba54a0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: total: 15.6 ms\n",
      "Wall time: 7.04 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "# calculate t(X)X and t(X)y for the two splits\n",
    "xtx_gam_s1, xty_gam_s1 = s1.rdd.map(lambda row: map_gam(row)).reduce(lambda a,b: (a[0]+b[0], a[1]+b[1]))\n",
    "xtx_gam_s2, xty_gam_s2 = s2.rdd.map(lambda row: map_gam(row)).reduce(lambda a,b: (a[0]+b[0], a[1]+b[1]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "a1adde0a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: total: 0 ns\n",
      "Wall time: 5.48 ms\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "bigS = get_bigS(knots, lam=0.0026366508987303583)\n",
    "xtx_gam2_s1 = xtx_gam_s1 + bigS\n",
    "xtx_gam2_s2 = xtx_gam_s2 + bigS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "fb845e82",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[True, False]"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "[np.all(xtx_gam_s1.T == xtx_gam_s1), np.all(xtx_gam2_s1.T == xtx_gam2_s1)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "e74b7b09",
   "metadata": {},
   "outputs": [],
   "source": [
    "# solve normal equations using LU decomposition since penalized matrix is not symmetric\n",
    "beta_hat_gam_s1 = np.linalg.solve(xtx_gam2_s1, xty_gam_s1)\n",
    "beta_hat_gam_s2 = np.linalg.solve(xtx_gam2_s2, xty_gam_s2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "b93a521f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: total: 31.2 ms\n",
      "Wall time: 3.73 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "# calculate predictions from additive model\n",
    "o1 = s1.rdd.map(lambda row:get_prediction_gam(row, beta_hat_gam_s2)).toDF(['index', \"X\", \"y\", \"yhat\"])\n",
    "o2 = s2.rdd.map(lambda row:get_prediction_gam(row, beta_hat_gam_s1)).toDF(['index', \"X\", \"y\", \"yhat\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "d0af8e2c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ROO split conformal predictive intervals\n",
    "o1 = o1.transform(ROOsplitconformal)\n",
    "o2 = o2.transform(ROOsplitconformal)\n",
    "\n",
    "# merge the splits\n",
    "res_gam = o1.union(o2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "ac5e1eb2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# evaluate results\n",
    "metrics_gam = res_gam.select(lit('additive').alias('model'),\n",
    "                             rmse(col('y'), col('yhat')).alias('rmse'),\n",
    "                             coverage(col('y'), col('lower'), col('upper')).alias('coverage'), \n",
    "                             avg_length(col('lower'), col('upper')).alias('avg_length'))\n",
    "\n",
    "metrics = metrics_lm.union(metrics_gam)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "9ffc342f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+------------------+--------+-----------------+\n",
      "|   model|              rmse|coverage|       avg_length|\n",
      "+--------+------------------+--------+-----------------+\n",
      "|  linear| 3.665820561412509|    0.95|8.922321162283405|\n",
      "|additive|3.6049689704612264|    0.95|8.787756129260648|\n",
      "+--------+------------------+--------+-----------------+\n",
      "\n",
      "CPU times: total: 0 ns\n",
      "Wall time: 38.7 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "metrics.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c95457ea",
   "metadata": {},
   "source": [
    "## "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
